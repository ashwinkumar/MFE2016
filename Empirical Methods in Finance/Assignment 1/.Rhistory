qplot(Period,y,data=gtAuto,main="log Auto Sales",xlab="",ylab="") +
geom_point(col=I("red"),size=I(3)) +
geom_line(col=I("blue"),linetype="dashed")
?qplot
Period
acf(gtAuto$y)
lmout_base=lm(y~back(y)+back(y,12),data=gtAuto)
lmSumm(lmout_base)
acf(lmout_base$resid)
lmout_trends=lm(y~back(y)+back(y,12)+suvs+insurance,data=gtAuto)
lmSumm(lmout_trends)
acf(lmout_trends$res)
gtAuto$y.lag1 = back(gtAuto$y) #1 period lagged sales
gtAuto$y.lag12 = back(gtAuto$y,12) #12 period lagged sales
n = length(gtAuto$y)
k=17 #start with only the first 17 observations and then add one by one
gtAuto$Actual=gtAuto$Baseline=gtAuto$Trends=rep(NA,n) #initialize columns
for (t in k:(n-1)) {
# roll forward the regressions to predict one-step ahead
reg1 = lm(y~y.lag1+y.lag12,data=gtAuto[1:t,])
reg2 = lm(y~y.lag1+y.lag12+suvs+insurance, data=gtAuto[1:t,])
t1 = t+1
gtAuto$Actual[t1]=gtAuto$y[t1]
gtAuto$Baseline[t1]=predict(reg1,newdata=gtAuto[t1,])
gtAuto$Trends[t1]=predict(reg2,newdata=gtAuto[t1,])
}
z = gtAuto[(k+1):n,c("Period","Actual","Baseline","Trends")]
qplot(Period,y=Actual, color = "Actual", data=z,geom="line",
main="Motor Vehicles and Parts",ylab="log(sales)") +
geom_line(aes(y = Baseline, color = "Baseline")) +
geom_line(aes(y = Trends, color = "Trends")) +
theme_bw()
ActualSales=gtAuto$sales[13:length(gtAuto$sales)]
out1  <- lm(usa ~ canada +  uk + australia + france + germany + japan)
out1  <- lm(usa ~ canada +  uk + australia + france + germany + japan, data=countryret)
data("countryret")
data("countryret")
out1  <- with(countryret, lm(usa ~ canada +  uk + australia + france + germany + japan))
out2 <-  with(countryret, lm(canada ~ uk + australia + france + germany + japan))
lmSumm(out2)
summary(out2)
out1$coefficients
summary(out1)
0.02257/0.002309
0.02257/0.069587
sqrt(0.3243422)
0.02257/0.069587
0.3243422^2
0.03228*sqrt(102)
0.03228*sqrt(102)
0.1051979/0.03228
1/3.258919
0.3068502*0.3068502
summary(out2)
lmSumm(out2)
anova(out2)
data("multicollinear")
data("multicolinear")
View(multicolinear)
with(multicolinear, cor(x,y))
corr1 <- with(multicolinear, cor(x1,x2))
corr1
data("multicolinear")
corr1 <- with(multicolinear, cor(x1,x2))
out <- lm(y~x1+x2)
lmSumm(out)
data("multicolinear")
corr1 <- with(multicolinear, cor(x1,x2))
out <- lm(y~x1+x2)
data("multicolinear")
corr1 <- with(multicolinear, cor(x1,x2))
out <- with(multicolinear,lm(y~x1+x2))
lmSumm(out)
anova(out)
?anova
data("detergent")
View(detergent)
View(detergent)
str(detergent)
str(detergent$stro)
str(detergent$store)
with(detergent, lm(log(q_tide128)~ log(p_tide128) + store))
out <- with(detergent, lm(log(q_tide128)~ log(p_tide128) + store))
out
View(detergent)
data("EquityPremium")
View(EquityPremium)
?attach
attach("EquityPremium")
data("EquityPremium")
attach(EquityPremium)
attach(EquityPremium)
EL_post1927 <- EquityPremium[yyyymm>1927]
EL_post1927 <- EquityPremium[yyyymm>1927,]
View(EL_post1927)
EL_post1927 <- EquityPremium[yyyymm>=192701,]
?EquityPremium
EP <- CRSP_SPvw - Rfree
mean(EP)
EP
mean(EP, nna.rm = "TRUE")
mean(EP, nna.rm = TRUE)
EP
rep <- 500
termvw <- double(rep)
termRfree <- double(rep)
for(r in 1:rep){
Retvw <- sample(CRSP_SPvw,size=25*12,replace = TRUE)
RetRfree <- sample(Rfree,suze=25*12, replace = TRUE)
termvw <- prod(1+Retvw)
termRfree <- prod(1+ RetRfree)
}
rep <- 500
termvw <- double(rep)
termRfree <- double(rep)
for(r in 1:rep){
Retvw <- sample(CRSP_SPvw,size=25*12,replace = TRUE)
RetRfree <- sample(Rfree,size=25*12, replace = TRUE)
termvw <- prod(1+Retvw)
termRfree <- prod(1+ RetRfree)
}
Retvw
ngrid=50
grid=seq(from=-5,to=5,len=ngrid)
nobs=10
X=matrix(runif(nobs,min=-2,max=2),ncol=1)
beta=1
sigma=3
X
ngrid=50
grid=seq(from=-7,to=8,len=ngrid)
nsample=10
plot(range(grid),c(-400,0),xlab="beta",ylab="lnlike",type="n")
betahat=double(nsample)
y=simlm(X,beta,sigma)
lnlike=function(y,X,beta,sigma)
{
n=length(y)
betahat=chol2inv(chol(crossprod(X)))%*%crossprod(X,y)
e=y-X%*%betahat
nu=n-ncol(X)
s=sqrt(sum(e**2)/nu)
vec=X%*%(beta-betahat)
return(-n*log(sigma)-.5*(nu*s**2+sum(vec**2)))
}
simlm=function(X,beta,sigma)
{
y=X%*%beta+rnorm(nrow(X),sd=sigma)
return(y)
}
col.alpha = function(color,alpha)
{
code = col2rgb(color)/256
rgb(code[1],code[2],code[3],alpha)
}
y=simlm(X,beta,sigma)
y
betahat[sample]=chol2inv(chol(crossprod(X)))%*%crossprod(X,y)
betahat[sample]=chol2inv(chol(crossprod(X,X)))%*%crossprod(X,y)
crossprod(X,y)
chol2inv(chol(crossprod(X)))
betahat[sample]=(chol2inv(chol(crossprod(X))))%*%crossprod(X,y)
betahat[sample] <- chol2inv(chol(crossprod(X)))%*%crossprod(X,y)
beta
betahat
for(sample in 1:nsample)
{
y=simlm(X,beta,sigma)
betahat[sample] <- chol2inv(chol(crossprod(X)))%*%crossprod(X,y)
ll=double(ngrid)
for (i in 1:ngrid)
{
ll[i]=lnlike(y=y,X=X,beta=grid[i],sigma=sigma)
}
lines(grid,ll,type="l",col=col.alpha("black",.7),lwd=1)
}
ngrid
ngrid
grid
rm(list = ls())
debugSource('~/Acads/MFE 2016/Econometrics/R/Ch_VI_Code_Snippets.R')
y=simlm(X,beta,sigma)
sample
nsample
r
for(sample in 1:nsample)
{
y=simlm(X,beta,sigma)
betahat[sample] <- chol2inv(chol(crossprod(X)))%*%crossprod(X,y)
ll=double(ngrid)
for (i in 1:ngrid)
{
ll[i]=lnlike(y=y,X=X,beta=grid[i],sigma=sigma)
}
lines(grid,ll,type="l",col=col.alpha("black",.7),lwd=1)
}
beta
beata
beta
print s
p s
s <-1
s**1
s**1
s**2
s**3
s <- 2
s**2
rm(list =ls())
sim.ARCHM=function(T,inita,alpha){
# function to simulate ARCHM
sigmasqt=function(atlag,alpha){
M=length(atlag)
return(alpha[1]+sum(alpha[2:(M+1)]*atlag**2))
}
M=length(inita)
a=double(T+M)
a[1:M]=inita
for(t in (M+1):(T+M)){
sd= sqrt(sigmasqt(a[(t-1):(t-M)],alpha))
a[t]=rnorm(1,sd=sd)
}
return(a[(M+1):(T+M)])
}
alpha=c(1,.7)
inita=c(0)
debugSource('~/Acads/MFE 2016/Econometrics/R/Ch_VI_Code_Snippets.R')
M=length(inita)
a=double(T+M)
a[1:M]=inita
for(t in (M+1):(T+M)){
sd= sqrt(sigmasqt(a[(t-1):(t-M)],alpha))
a=sim.ARCHM(1100,inita,alpha)[101:1100]
a=sim.ARCHM(1100,inita,alpha)[101:1100]
a=sim.ARCHM(1100,inita,alpha)[101:1100]
rm(list= ls())
sim.ARCHM=function(T,inita,alpha){
# function to simulate ARCHM
sigmasqt=function(atlag,alpha){
M=length(atlag)
return(alpha[1]+sum(alpha[2:(M+1)]*atlag**2))
}
M=length(inita)
a=double(T+M)
a[1:M]=inita
for(t in (M+1):(T+M)){
sd= sqrt(sigmasqt(a[(t-1):(t-M)],alpha))
a[t]=rnorm(1,sd=sd)
}
return(a[(M+1):(T+M)])
}
alpha=c(1,.7)
inita=c(0)
a=sim.ARCHM(1100,inita,alpha)[101:1100]
size(a)
size[a]
length(a)
a=sim.ARCHM(1100,inita,alpha)
length(a)
M=length(inita)
a=double(T+M)
T
T<-1100
M=length(inita)
a=double(T+M)
alpha=c(1,.7)
inita=c(0)
T <- 1100
M=length(inita)
a=double(T+M)
a[1:M]=inita
t <- 2
sd= sqrt(sigmasqt(a[(t-1):(t-M)],alpha))
sigmasqt=function(atlag,alpha){
M=length(atlag)
return(alpha[1]+sum(alpha[2:(M+1)]*atlag**2))
}
sd= sqrt(sigmasqt(a[(t-1):(t-M)],alpha))
?optim
rm(list = ls())
data(lmich_yr)
with(lmich_yr, plot(back(Level), (Level)))
library("DataAnalytics", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
data(lmich_yr)
with(lmich_yr, plot(back(Level), (Level)))
out <- with(lmich_yr,lm(Level~back(Level)))
corr <- with(lmich_yr,cor(Level,back(Level), use = "complete"))
out
corr
lmSumm(out)
?qt
pt(0.5,10)
pt(0,10)
#Empirical method HW1 Problem 2
#part 1
#setwd("C:/Users/SallyShi/Desktop/MGMT237E-Empirical Methods in Finance/HW1")
library(xts)
Fund <- read.xls("DBV.xlsx",  na.strings = FALSE )
SP <- read.xls("GSPC.xlsx",  na.strings = FALSE )
Fund[,1]<-as.Date(Fund[,1],format("%m/%d/%Y"))
#create xts object
Funddata<-as.xts(Fund[,7],order.by=Fund[,1])
Fundret<-diff(log(Funddata),lag=1)
Fundret<-Fundret[-1,]
head(Fundret)
SP[,1]<-as.Date(SP[,1],format("%m/%d/%Y"))
SPdata<-as.xts(SP[,7],order.by=SP[,1])
SPret<-diff(log(SPdata),lag=1)
SPret<-SPret[-1,]
#time-series plot of daily log-returns
plot(Fundret,main="Time series plot of DBV log return")
plot(SPret,main="Time series plot of GSPC log return")
hist(Fundret,main="DBV log return histogram",breaks=40)
hist(SPret,main="GSPC log return histogram",breaks=40)
#part 2
library(fBasics)
basicStats(Fundret)
t3<-function(ret){
S3<-skewness(ret)
T<-length(ret)
t3<-S3/sqrt(6/T)
t3
}
t4<-function(ret){
K4<-kurtosis(ret)
T<-length(ret)
t4<-K4/(sqrt(24/T))
t4
}
t3(Fundret)
t4(Fundret)
normalTest(Fundret,method='jb')
criticalVal=abs(qnorm(0.05/2))
#abs(t3(Fundret))>criticalVal
#reject the null hypothesis, log return of DBV is negatively skewed at 5% significant level
#t4(Fundret)>criticalVal
#reject the null hypothesis, log return of DBV has heavy tails at 5% significant level
#Jarque-Bera test Asymptotic p Value: < 2.2e-16
#reject the null hypothesis at 5% significant level
basicStats(SPret)
t3(SPret)
t4(SPret)
normalTest(SPret,method='jb')
normalTest(SPret,method='jb')@test$p.value
#abs(t3(SPret))>criticalVal
#reject the null hypothesis, log return of GSPC is negatively skewed at 5% significant level
#t4(Fundret)>criticalVal
#reject the null hypothesis, log return of GSPC has heavy tails at 5% significant level
#Jarque-Bera test Asymptotic p Value: < 2.2e-16
#reject the null hypothesis at 5% significant level
#part 3
stats<-data.frame(cbind(basicStats((Fundret)),basicStats((SPret))))
table<-rbind(stats["Mean",],stats["Variance",],stats["Stdev",],stats["Skewness",],stats["Kurtosis",])
table["skewness test Tvalue",]<-cbind(t3(Fundret),t3(SPret))
table["kurosis test Tvalue",]<-cbind(t4(Fundret),t4(SPret))
table["Jarque-Bera test Pvalue",]<-cbind(normalTest(Fundret,method='jb')@test$p.value,normalTest(SPret,method='jb')@test$p.value)
colnames(table)=c("DBV","GSPC")
table
rm(list = ls())
library(devtools)
library(gdata)
library(DataAnalytics)
library(graphics)
library(timeSeries)
#part 1
DBV_data <- read.xls("DBV.xlsx",  na.strings = FALSE )
DBV_data$Date <- as.Date(DBV_data$Date)
#DBV_data <- as.xts(DBV_data , order.by = DBV_data$Date)
DBV_close <- as.xts(DBV_data$Adj, order.by = DBV_data$Date)
DBV_log_return <- log(DBV_close)- log(lag(DBV_close))
#remove na
DBV_log_return <- DBV_log_return[!is.na(DBV_log_return)]
GSPC_data <- read.xls("GSPC.xlsx",  na.strings = FALSE )
GSPC_data$Date <- as.Date(GSPC_data$Date)
#GSPCdata <- as.xts(GSPCdata , order.by = GSPCdata$Date)
GSPC_close <- as.xts(GSPC_data$Adj, order.by = GSPC_data$Date)
GSPC_log_return <- log(GSPC_close)- log(lag(GSPC_close))
#remove na
GSPC_log_return <- GSPC_log_return[!is.na(GSPC_log_return)]
par(mfrow = c(2,2))
plot(DBV_log_return ,main = "Log Returns of DBV")
plot(GSPC_log_return ,main = "Log Returns of GSPC")
hist(DBV_log_return, breaks = 50,freq = FALSE, xlab= "Log Returns of DBV")
hist(GSPC_log_return,breaks = 50,  freq = FALSE ,xlab = "Log Returns of GSPC")
#part 2
library(fBasics)
#basicStats(DBV_log_return)
t_skewnewss <- function(data){
skewness(data, na.rm = TRUE)/sqrt(6/length(data))
}
t_kurtosis <- function(data){
(kurtosis(data, na.rm = TRUE))/sqrt(24/length(data))
}
#a) Test the null that the skewness of daily log returns is zero at the 5% significance level
t_skew_DBV <- abs(t_skewnewss(DBV_log_return))
t_skew_GSPC <-abs(t_skewnewss(GSPC_log_return))
t_critical <- abs(qnorm(0.05/2))
#Since t_skew_DBV and t_skew_GSPC > t_critical, we reject the null at 5% significance level
#b)Test the null that the excess kurtosis of daily log returns is zero at the 5% significancelevel.
t_kurt_DBV <- t_kurtosis(DBV_log_return)
t_kurt_GSPC <- t_kurtosis(GSPC_log_return)
#Since t_kurt_DBV and t_kurt_GSPC > t_critical, we reject the null at 5% significance level
#c) Test the null that the daily log returns are normally distributed at the 5% significance level using the Jarque-Bera test.
jb_DBV <- t_skew_DBV^2 + t_kurt_DBV^2
jb_GSPC <- t_skew_GSPC^2 + t_kurt_GSPC^2
chisq <- qchisq(0.05,2)
p_jb_DBV <- normalTest(as.vector(DBV_log_return), method ='jb')@test$p.value
p_jb_GSPC <-normalTest(as.vector(GSPC_log_return),method='jb')@test$p.value
#3
stats<-data.frame(cbind(basicStats((DBV_log_return)),basicStats((GSPC_log_return))))
table<-rbind(stats["Mean",],stats["Variance",],stats["Stdev",],stats["Skewness",],stats["Kurtosis",])
table["skewness test Tvalue",]<-cbind(t_skew_DBV,t_skew_GSPC)
table["kurosis test Tvalue",]<-cbind(t_kurt_DBV,t_kurt_GSPC)
table["Jarque-Bera test Pvalue",]<-cbind(p_jb_DBV,p_jb_GSPC)
colnames(table)=c("DBV","GSPC")
table
rm(list = ls())
library(fBasics)
#basicStats(DBV_log_return)
t_skewnewss <- function(data){
skewness(data, na.rm = TRUE)/sqrt(6/length(data))
}
t_kurtosis <- function(data){
(kurtosis(data, na.rm = TRUE))/sqrt(24/length(data))
}
#a) Test the null that the skewness of daily log returns is zero at the 5% significance level
t_skew_DBV <- abs(t_skewnewss(DBV_log_return))
t_skew_GSPC <-abs(t_skewnewss(GSPC_log_return))
t_critical <- abs(qnorm(0.05/2))
#Since t_skew_DBV and t_skew_GSPC > t_critical, we reject the null at 5% significance level
#b)Test the null that the excess kurtosis of daily log returns is zero at the 5% significancelevel.
t_kurt_DBV <- t_kurtosis(DBV_log_return)
t_kurt_GSPC <- t_kurtosis(GSPC_log_return)
library(devtools)
library(gdata)
library(DataAnalytics)
library(graphics)
library(timeSeries)
#part 1
DBV_data <- read.xls("DBV.xlsx",  na.strings = FALSE )
DBV_data$Date <- as.Date(DBV_data$Date)
setwd("~/Acads/MFE 2016/2nd Quarter/Empirical Methods in Finance/Week 1/HW")
library(devtools)
library(gdata)
library(DataAnalytics)
library(graphics)
library(timeSeries)
#part 1
DBV_data <- read.xls("DBV.xlsx",  na.strings = FALSE )
DBV_data$Date <- as.Date(DBV_data$Date)
#DBV_data <- as.xts(DBV_data , order.by = DBV_data$Date)
DBV_close <- as.xts(DBV_data$Adj, order.by = DBV_data$Date)
DBV_log_return <- log(DBV_close)- log(lag(DBV_close))
#remove na
DBV_log_return <- DBV_log_return[!is.na(DBV_log_return)]
GSPC_data <- read.xls("GSPC.xlsx",  na.strings = FALSE )
GSPC_data$Date <- as.Date(GSPC_data$Date)
#GSPCdata <- as.xts(GSPCdata , order.by = GSPCdata$Date)
GSPC_close <- as.xts(GSPC_data$Adj, order.by = GSPC_data$Date)
GSPC_log_return <- log(GSPC_close)- log(lag(GSPC_close))
#remove na
GSPC_log_return <- GSPC_log_return[!is.na(GSPC_log_return)]
par(mfrow = c(2,2))
plot(DBV_log_return ,main = "Log Returns of DBV")
plot(GSPC_log_return ,main = "Log Returns of GSPC")
hist(DBV_log_return, breaks = 50,freq = FALSE, xlab= "Log Returns of DBV")
hist(GSPC_log_return,breaks = 50,  freq = FALSE ,xlab = "Log Returns of GSPC")
#part 2
library(fBasics)
#basicStats(DBV_log_return)
t_skewnewss <- function(data){
skewness(data, na.rm = TRUE)/sqrt(6/length(data))
}
t_kurtosis <- function(data){
(kurtosis(data, na.rm = TRUE))/sqrt(24/length(data))
}
#a) Test the null that the skewness of daily log returns is zero at the 5% significance level
t_skew_DBV <- abs(t_skewnewss(DBV_log_return))
t_skew_GSPC <-abs(t_skewnewss(GSPC_log_return))
t_critical <- abs(qnorm(0.05/2))
#Since t_skew_DBV and t_skew_GSPC > t_critical, we reject the null at 5% significance level
#b)Test the null that the excess kurtosis of daily log returns is zero at the 5% significancelevel.
t_kurt_DBV <- t_kurtosis(DBV_log_return)
t_kurt_GSPC <- t_kurtosis(GSPC_log_return)
#Since t_kurt_DBV and t_kurt_GSPC > t_critical, we reject the null at 5% significance level
#c) Test the null that the daily log returns are normally distributed at the 5% significance level using the Jarque-Bera test.
jb_DBV <- t_skew_DBV^2 + t_kurt_DBV^2
jb_GSPC <- t_skew_GSPC^2 + t_kurt_GSPC^2
chisq <- qchisq(0.05,2)
p_jb_DBV <- normalTest(as.vector(DBV_log_return), method ='jb')@test$p.value
p_jb_GSPC <-normalTest(as.vector(GSPC_log_return),method='jb')@test$p.value
#3
stats<-data.frame(cbind(basicStats((DBV_log_return)),basicStats((GSPC_log_return))))
table<-rbind(stats["Mean",],stats["Variance",],stats["Stdev",],stats["Skewness",],stats["Kurtosis",])
table["skewness test Tvalue",]<-cbind(t_skew_DBV,t_skew_GSPC)
table["kurosis test Tvalue",]<-cbind(t_kurt_DBV,t_kurt_GSPC)
table["Jarque-Bera test Pvalue",]<-cbind(p_jb_DBV,p_jb_GSPC)
colnames(table)=c("DBV","GSPC")
table
rm(list = ls())
library(xts)
Fund <- read.xls("DBV.xlsx",  na.strings = FALSE )
SP <- read.xls("GSPC.xlsx",  na.strings = FALSE )
DBV_data <- read.xls("DBV.xlsx",  na.strings = FALSE )
DBV_data$Date <- as.Date(DBV_data$Date)
#DBV_data <- as.xts(DBV_data , order.by = DBV_data$Date)
DBV_close <- as.xts(DBV_data$Adj, order.by = DBV_data$Date)
DBV_log_return <- log(DBV_close)- log(lag(DBV_close))
#remove na
View(DBV_log_return)
0.000015/sqrt(0.000077)
0.000188/sqrt(0.000181)
